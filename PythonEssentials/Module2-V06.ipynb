{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b988034e",
   "metadata": {},
   "source": [
    "Understanding 3rd Party Packaging\n",
    "Summary\n",
    "\n",
    "This\n",
    " template\n",
    " provides a starting point for ML/DL projects leveraging GPU acceleration. It uses mainstream Python tools like virtualenv, pip, Docker, PyTorch, and TensorFlow to configure a development environment with GPU access, isolate dependencies, and test GPU training.\n",
    "\n",
    "Top 4 Key Points\n",
    "\n",
    "Check virtualenv is active to manage packages separately\n",
    "\n",
    "Dockerfiles included to build GPU container images\n",
    "\n",
    "PyTorch and TensorFlow tests validate GPU works\n",
    "\n",
    "Tools like BentoML and Hugging Face integrate nicely\n",
    "\n",
    "5 Reflection Questions\n",
    "\n",
    "How could a Makefile streamline training model experiments?\n",
    "\n",
    "Why isolate Python dependencies in virtualenvs and containers?\n",
    "\n",
    "What role do GitHub Actions play in an ML ops pipeline?\n",
    "\n",
    "How could BentoML serve models for low-latency requests?\n",
    "\n",
    "When would fine-tuning a Hugging Face model be preferred over training from scratch?\n",
    "\n",
    "5 Challenge Exercises\n",
    "\n",
    "Adjust hyperparameters in the PyTorch GPU test code\n",
    "\n",
    "Log GPU usage with nvidia-smi during model training\n",
    "\n",
    "Build a Docker container to run TensorFlow code\n",
    "\n",
    "Serve a scikit-learn model with BentoML locally\n",
    "\n",
    "Fine-tune DistilBERT model on a small text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1dc0c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
