{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b988034e",
   "metadata": {},
   "source": [
    "Understanding 3rd Party Packaging\n",
    "Summary\n",
    "\n",
    "This\n",
    " template\n",
    " provides a starting point for ML/DL projects leveraging GPU acceleration. It uses mainstream Python tools like virtualenv, pip, Docker, PyTorch, and TensorFlow to configure a development environment with GPU access, isolate dependencies, and test GPU training.\n",
    "\n",
    "## Top 4 Key Points\n",
    "\n",
    "- Check virtualenv is active to manage packages separately\n",
    "\n",
    "- Dockerfiles included to build GPU container images\n",
    "\n",
    "- PyTorch and TensorFlow tests validate GPU works\n",
    "\n",
    "- Tools like BentoML and Hugging Face integrate nicely\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "ðŸ”¹ 1. How could a Makefile streamline training model experiments?\n",
    "\n",
    "A Makefile provides a standardized way to define commands as targets. In ML experiments, there are usually repetitive steps such as:\n",
    "\n",
    "setting up data,\n",
    "\n",
    "preprocessing,\n",
    "\n",
    "training,\n",
    "\n",
    "evaluating,\n",
    "\n",
    "cleaning checkpoints.\n",
    "\n",
    "By writing these as Makefile rules (e.g., make train, make eval), you:\n",
    "\n",
    "avoid remembering long shell commands,\n",
    "\n",
    "enforce consistency across different runs,\n",
    "\n",
    "enable reproducibility by documenting the exact workflow.\n",
    "\n",
    "This makes collaboration easier tooâ€”anyone on the team can run the same experiments with make.\n",
    "\n",
    "ðŸ”¹ 2. Why isolate Python dependencies in virtualenvs and containers?\n",
    "\n",
    "ML projects often rely on specific versions of TensorFlow, PyTorch, CUDA, scikit-learn, etc. If you install everything globally:\n",
    "\n",
    "version conflicts arise between projects,\n",
    "\n",
    "reproducing results becomes difficult,\n",
    "\n",
    "system packages might break.\n",
    "\n",
    "Using virtualenvs (or venv) ensures project-level isolation.\n",
    "Using containers (e.g., Docker) goes further by isolating not just Python but the entire OS environment, including CUDA drivers and system libraries.\n",
    "\n",
    "ðŸ‘‰ This guarantees that training runs on one machine (or in production) behave the same on another.\n",
    "\n",
    "ðŸ”¹ 3. What role do GitHub Actions play in an ML Ops pipeline?\n",
    "\n",
    "GitHub Actions act as the automation backbone of an ML pipeline:\n",
    "\n",
    "CI/CD for ML code: test code, linting, style checks.\n",
    "\n",
    "Data and model workflows: trigger training jobs when new data is pushed.\n",
    "\n",
    "Model validation: automatically evaluate models on benchmarks before merging.\n",
    "\n",
    "Deployment: push trained models to a registry or cloud service when tests pass.\n",
    "\n",
    "This removes manual steps, reduces human error, and ensures continuous, reliable delivery of ML models.\n",
    "\n",
    "ðŸ”¹ 4. How could BentoML serve models for low-latency requests?\n",
    "\n",
    "BentoML is designed for model serving. It wraps trained models into a standardized API service (REST/gRPC) with:\n",
    "\n",
    "efficient model loading,\n",
    "\n",
    "optimized inference pipelines,\n",
    "\n",
    "autoscaling support.\n",
    "\n",
    "For low-latency inference (e.g., fraud detection, personalized recommendations), BentoML:\n",
    "\n",
    "keeps the model in memory,\n",
    "\n",
    "handles concurrent requests,\n",
    "\n",
    "integrates with GPU/CPU optimizations,\n",
    "\n",
    "supports containerization for deployment.\n",
    "\n",
    "This makes it production-ready without needing to manually write Flask/FastAPI wrappers.\n",
    "\n",
    "ðŸ”¹ 5. When would fine-tuning a Hugging Face model be preferred over training from scratch?\n",
    "\n",
    "Fine-tuning a pre-trained Hugging Face model (e.g., BERT, GPT-2, ViT) is preferred when:\n",
    "\n",
    "Data is limited â†’ you leverage knowledge from massive pretraining corpora.\n",
    "\n",
    "Domain transfer is needed â†’ e.g., fine-tuning BERT on medical text (BioBERT) instead of training from scratch.\n",
    "\n",
    "Compute efficiency â†’ training from scratch on billions of tokens/images requires GPUs/TPUs at scale.\n",
    "\n",
    "Training from scratch is only justified when:\n",
    "\n",
    "you have a huge, domain-specific dataset,\n",
    "\n",
    "or the existing pre-trained models are fundamentally mismatched with your task.\n",
    "\n",
    "## Challenge Exercises\n",
    "\n",
    "Adjust hyperparameters in the PyTorch GPU test code\n",
    "\n",
    "Log GPU usage with nvidia-smi during model training\n",
    "\n",
    "Build a Docker container to run TensorFlow code\n",
    "\n",
    "Serve a scikit-learn model with BentoML locally\n",
    "\n",
    "Fine-tune DistilBERT model on a small text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1dc0c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
